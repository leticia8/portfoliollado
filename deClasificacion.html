<!DOCTYPE HTML>
<!--
Editorial by HTML5 UP
html5up.net | @ajlkn
Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
<head>
<title>Classification</title>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
<link rel="stylesheet" href="assets/css/main.css" />
</head>
<body class="is-preload">

<!-- Wrapper -->
<div id="wrapper">

<!-- Main -->
	<div id="main">
		<div class="inner">

			<!-- Header -->
				<header id="header">
					<a href="index.html" class="logo"></a>
					<ul class="icons">
							<li><a href="https://github.com/leticia8/portofliollado" class="icon brands fa-github"><span class="label">Git Hub</span></a></li>
					<li><a href="https://www.linkedin.com/in/leticia-lado-50b7a5106/" class="icon brands fa-linkedin"><span class="label">Linked In</span></a></li>
			</ul>
				</header>
				
			<!-- Content -->
				<section>
					<header class="main">
						<h1>Classification algorithms</h1>
					</header>

					<span class="image main"><img src="images/supClas.png" style="width:60%;height:80%;" alt="" /></span>

					<p>Classification problems are those in which the output variable is a categorical class such as "red" or "green", "high" and "low". Some
						algorithms present better performances for binominal classes while others serve for multiple classes. </p>
						Among the most used classification algorithms are: </p>



				<p style="font-size:24px ;color:Tomato;" > LOGISTIC REGRESSION </p>
							<p style="font-size:18px ;text-align:justify" >
								It is an algorithm for binary classification problems, that is, its output determines whether or not it belongs to a class.
								The algorithm uses a sigmoid function that results in an S-shaped curve that can take any real value and
								map it and values from 0 to 1 but never exactly those values.
								Models the probability of the default class, that is, the probability that an input value X belongs to the default class Y = 1:<br><br>								
									<img src="http://latex.codecogs.com/gif.latex?P(X)=P(Y=1|X)" border="0"/>. <br>

									It is a linear method, but the predictions are transformed using the logistic function, while by means of the maximum likelihood estimation, the coefficients of the algorithm are calculated.<br><br>
							<strong>DATA PREPARATION: </strong><br>
							<ul>Remove noise</ul>
							<ul>Ensure Gaussian distribution: Assume linear relationship between variables</ul>
							<ul>Remove correlated input variables or if the information is highly differentiated it may fail to converge. </ul>
							<ul>It can also have low performance due to a small amount of data.</ul></p>
							
							
							<p style="font-size:24px ;color:Tomato;" >LINEAR DISCRIMINANT ANALYSIS</p>

							<p style="font-size:18px ;text-align:justify" >If we have more than two classes, it is the preferred classification technique, it can also be applied with binary classification problems. <br>
								Statistical descriptors are calculated for each class, for a single input variable these can be mean and variance for each class.
								Means and covariance matrices are used for many variables.<br>
								The LDA method assumes that the data has a normal distribution and each attribute has the same variance. <br>
								The sum of the squared differences of each value to the mean within the reference class is calculated, but the average is performed using all groups as reference. <br>
								LDA predicts by estimating the probability that a new set of points belong to each class.
								The class that gets the highest probability is the output class. The algorithm uses Bayes' theorem to estimate these probabilities.
								<br><br>
								<strong>DATA PREPARATION </strong><br>
								Supports binary and multiclass classifications.<br>
								Normal distribution: normal implementation assumes normal distribution for input variables.<br>
							Remove outliers:<br>
							Same variance: LDA assumes that each input variable has the same variance. It is good to standardize the data with mean 0 and variance 1 before applying the algorithm.<br>
							<p style="font-size:24px ;color:Tomato;" >SUPPORT VECTOR MACHINE - SVM</p>

							<p style="font-size:18px ;text-align:justify" >The hyperplane that best separates the points in the input variable space is selected.
								It could be displayed as a line, and it is assumed that all the values of the input variables can be completely separated by this line.<br>
								Using this line it is possible to determine if a new point to be classified is above or below the line depending on whether it returns positive or negative values; in the first case, it returns the value 0 and in the second, 1.
								A value close to the line can be difficult to classify.<br>
							The margin is calculated as the orthogonal distance to the line for the closest points. Only these points are relevant for defining the line and for constructing the classifier. These points are called support vectors and define the hyperplane.<br>
							
							There is an adaptation that softens the strictness of the margin, allowing some points to violate the separation of the line. A tuning parameter C is defined that refers to the number of points that are tolerated that violate this margin. A C = 0 implies no violation and is the one adopted by the Maximal-Margin Classifier method. The smaller the value of C, the algorithm is more sensitive to training values.<br>
							<strong>KERNEL SVM: </strong><br>
							In practice the algorithm is implemented using a kernel or kernel. Said Kernel function can be:
							Linear:<br>
							<img src="http://latex.codecogs.com/gif.latex?K(x, x_i)=\sum(x * x_i)" border="0"/>. <br>
							Polinomial:<br>
									<img src="http://latex.codecogs.com/gif.latex?K(x,x_i) = 1 + \sum(x * x_i)^d" border="0"/>. <br>
							 
							
							Radial:<br>
							<img src="http://latex.codecogs.com/gif.latex?K(x,x_i) = e ^ {-gamma *\sum(x * x_i^2)}" border="0"/>. <br><br>
						
							<strong>DATA PREPARATION: </strong><br>
							The values ​​of the input variables must be numeric, if they are categorical they will have to be transformed to dummy variables (value 1 - 0). 
							It is a method ideally thought for binary classification.</p>
							
							<p style="font-size:24px ;color:Tomato;" >NAIVE BAYES</p>
							<p style="font-size:18px ;text-align:justify" >The Naive Bayes theorem is based on the use of the researcher's prior knowledge on a certain topic.
								Bayes' theorem is:<br><br>
							<img src="http://latex.codecogs.com/gif.latex?P(h|d) = \frac{P(d|h) * P(h)}{P(d)}" border="0"/>. <br><br>
							

							Where:
							P(h│d) is the probability of a hypothesis h given the information d. This is called a posteriori probability.<br>
							P(d│h) is the probability of the information d given that the hypothesis h is true.<br>
							P(h) is the probability of hypothesis h to be true. It is the a priori probability.<br>
							P(d) is the probability of the data<br>

							Naive Bayes is a classification algorithm for binary and multiclass problems. The technique is the easiest to understand when it is described using binary or categorical variables. <br>
							The probability values of each attribute are assumed to be conditionally independent given the target value to be calculated. <br>
							The representation of the Naive Bayes method is a list of stored probabilities that includes class probabilities and conditional probabilities. <br>
							The training is fast because only the probability of each class and that of each class given the input value must be calculated, it is not necessary to adjust coefficients or optimization procedures. <br>
							The algorithm can be extended to real attributes, assuming normal distributions, other distribution functions can also be used but the Gaussian distribution is the easiest way to work because it only needs to estimate mean and standard deviation.<br><br>
							<strong>DATA PREPARATION: </strong><br>
							Categorical input variables: assumes that the attributes to be estimated are binary, categorical or nominal.
							Normal distribution: if the input variables are real, normal distribution is assumed.
							Classification algorithm used to classify into two or more classes.
							Logarithmic probabilities: It is useful to perform a logarithmic transformation of the probabilities in order to avoid precision overflow.
							More complex distributions as well as a variety of kernel density functions can be used for different data distributions.</p>


							<p style="font-size:24px ;color:Tomato;" >DECISION TREES - CART - Classification And Regression Trees </p>
							<p style="font-size:18px ;text-align:justify" >They have classically been referred to as decision trees although today they are called Classification and Regression Trees (CART)<br>
						
								The representation is a binary tree where each node represents an input variable x and a cut point on that variable. The leaves of the tree contain the output variable (y) that are used to make the prediction. <br>
								Making predictions is pretty straightforward.
								Each input variable can be thought of as a dimension in p-dimensional space.
								The decision tree divides the space into two rectangles or hyper-rectangles with more entries. <br>
								An avid method is used to divide the space called binary recursive division.
								It is a numerical procedure where all the values are aligned and different division points are tested using a cost function.
								The division with the best cost is selected.<br>
							
								For classification problems the Gini cost is used and provides an indication of how pure the nodes are, values close to 0 have all classes of the same type. <br>
								For classification algorithms such as bagging, each tree votes and the proportion of votes in each class is the estimated probability error. <br>
								The most common stopping procedure is to use the minimum amount of training data used for each leaf node. <br>
								The tree can be pruned after training since simpler trees are preferred since they are easier to understand and less likely to overfit information. <br>
								The quickest and simplest pruning methods are carried out by evaluating the effect on the cost of removing said node from the model.
								When no improvements are achieved, the procedure is stopped.<br><br>
							<strong>DATA PREPARATION: </strong><br>
								No required.<br></p>

							
							<p style="font-size:24px ;color:Tomato;" >RANDOM FOREST</p>
							<p style="font-size:18px ;text-align:justify"> They are an improvement over decision trees that use the Bagged (Bagging and Aggregation) algorithm.
								The problem is that it is an avid algorithm, this implies that in each iteration it looks for the local optimum that may not be the optimum at a general level.
								In each split, a number of attributes established by parameter are evaluated that are randomly selected from among all the available attributes.
								For classification problems it is advisable to use the square root of the number of predictors as a parameter that determines how many random attributes to select. <br>
								Random Forest solves the problem of correlation between models by including randomness in their elaboration.
								To decide the number m of attributes to select, it is better to start with 5 values between 2 and p (number of attributes or input variables) and go testing the performance, for the number of models to run.
								An approximate 1000 trees is recommended, although it depends on the problem and knowledge of the business. 
								<br>
								In random forest all trees are created independently to have the desired depth and contribute equally to the model.
								<br></p>
						




					<hr class="major" />

								<h2>Projects using classification methods</h2>
						<ul>
										<li><a href="proyectoDos.html">Cardiology prediction</a></li>
										<li><a href="proyectoTres.html"></a>Prediction of patients with / without liver</li>
										<li><a href="proyectoCinco.html">Prediction of autism in adults</a></li>
									</ul>
					<h2> </h2>
					

				</section>

		</div>
	</div>

	<!-- Sidebar -->
		<div id="sidebar">
			<div class="inner">

				<!-- Search -->
					<section id="search" class="alt">
						<form method="post" action="#">
							<input type="text" name="query" id="query" placeholder="Search" />
						</form>
					</section>
	<!-- Menu -->
		<nav id="menu">
			<header class="major">
				<h2>Menu</h2>
			</header>
			<ul>
				<li><a href="index.html">Home</a></li>
				<li><a href="introduccion.html">Introduction to Machine Learning </a></li>
				<li>
					<span class="opener" href="index.html">Theoretical framework</span>
					<ul>
						<li><a  href="supervisados.html">Supervised learning</a></li>
						<ul>
						<li><a href="deRegresion.html">Regression</a></li>
						<li><a href="deClasificacion.html">Classification</a></li>
						</ul>
						<li><a  href="noSupervisados.html">Unsupervised learning</a></li>
						<ul>
						<li><a href="deClustering.html">Clustering</a></li>
						<li><a href="deReduccionDim.html">Reduction of dimensions</a></li>
						
						</ul>
					</ul>
				</li>
				<li>
					<span class="opener" href="proyectos.html">Projects</span>
					<ul>
						<li><a href="proyectoUno.html">Home price prediction  </a></li>
						<li><a href="proyectoDos.html">Cardiology prediction</a></li>
					
						<li><a href="proyectoCuatro.html">	Height prediction from skeletal remains</a></li>
						<li><a href="proyectoTres.html">Prediction of patients with / without liver</a></li>
						<li><a href="proyectoCinco.html">Prediction of autism in adults</a></li>
				
			

					</ul>
				</li>
			
			</ul>
		</nav>

		
			
		</div>
	</div>

</div>

						</div>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>