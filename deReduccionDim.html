<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
	<title>Reduction of dimensions</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<link rel="stylesheet" href="assets/css/main.css" />
</head>

<body class="is-preload">

	<!-- Wrapper -->
	<div id="wrapper">

		<!-- Main -->
		<div id="main">
			<div class="inner">

				<!-- Header -->
				<header id="header">
					<a href="index.html" class="logo"></a>
					<ul class="icons">
						<li><a href="https://github.com/leticia8/portofliollado" class="icon brands fa-github"><span
									class="label">Git Hub</span></a></li>
						<li><a href="https://www.linkedin.com/in/leticia-lado-50b7a5106/"
								class="icon brands fa-linkedin"><span class="label">Linked In</span></a></li>
					</ul>
				</header>

				<!-- Content -->
				<section>
					<header class="main">
						<h1>Dimension reduction algorithms</h1>
					</header>

					<span class="image main"><img src="images/unsupDimRed.png" style="width:80%;height:60%;"
							alt="" /></span>

					<p style="font-size:20px ;text-align:justify">The problems of association or reduction of dimensions
						seek to discover rules that apply to a large proportion of the data,
						as for example people who buy A usually buy B.</p>

					<span class="image main"><img src="images/featureSel.png" style="width:90%;height:90%;"
							alt="" /></span>

					<p style="font-size:20px ;text-align:justify">Dimension reduction techniques include: <br>


					<p style="font-size:24px ;color:Tomato;"> FEATURE SELECTION</p>

					<p style="font-size:20px ;text-align:justify"> There are two types of attribute selection process
						selection: <br>
						Filter and wrapper. The filter ones select those attributes that meet certain established
						requirements
						while the wrappers iteratively select only the attributes that improve the performance of the
						algorithm. <br>
						Filter algorithms do not require any learning algorithm, while wrapper algorithms are optimized
						for a particular algorithm.<br>

						Filter algorithms can be considered "Unsupervised" and are used when there are many attributes
						or the cost
						computational is very high.<br> </p>

					<p style="font-size:24px ;color:Tomato;"> PRINCIPAL COMPONENT ANALYSIS- PCA</p>
					<span class="image main"><img src="images/pca.png" style="width:50%;height:50%;" alt="" /></span>

					<p style="font-size:20px ;text-align:justify"> PCA helps us answer the question of how many of the
						attributes in my data set explain a significant amount of the variation
						contained in the data set. In general, the 80/20 rule applies, 20% of the attributes explain 80%
						of the variation.
						PCA captures the attributes that contain the greatest amount of variability in the data set,
						transforming the variables
						existing in a set of "principal components" or new variables that have the following properties:
						<br>
						They are not correlated with each other <br>
						They explain a lot of the variance in the data. <br>
						They can be related to the original variables by the weights. <br>
						Variables with low weighting coefficient factors in their principal components are removed from
						the data set. <br>
						By means of a linear combination of the variables, a principal component z is created. <br>



						<img src="http://latex.codecogs.com/gif.latex?zm =\sum(w_i * x_i)" border="0" />. <br>

						An analysis of the eigenvalues of the covariance matrix of the original attributes is carried
						out. <br>
						The eigenvector associated with the largest eigenvalue is the first principal component, the
						vector associated with the second
						greater value of eigenvalues is the second principal component and so on. <br>
						Covariance explains how two variables vary with respect to their means. <br>
						PCA only works with numeric attributes. <br>
						Before eliminating any variable, an evaluation analysis of the data must be carried out in the
						context of the problem,
						if there is noise PCA can suggest as relevant variables those that are not, since it considers
						the variation. <br>
						It does not help to add irrelevant data or uncorrelated data. <br>
						PCA is an algorithm sensitive to the ranges of the variables, to minimize this the information
						must be normalized. <br>
					</p>



					<p style="font-size:24px ;color:Tomato;">WRAPPER METHODS </p>
					<p style="font-size:20px ;text-align:justify"> The "wrapping" approaches iteratively select items to
						add or remove from the data set based on whether the new information
						improves model accuracy. <br>
						One way to do this is to start with a variable, and build a base model. Then add a second
						variable and compare
						with the previous base model. If the performance is better, we make this model the new base
						model, we add a third variable
						and we follow the same procedure. If there is no improvement, we select a new attribute
						discarding the previous one and continue testing. <br>
						This algorithm is called "forward selection". <br>
						The reverse way is to start with all the attributes and remove one variable at a time by
						building a new model. In general
						The variable to be removed is selected considering the value of the t statistic, the one with
						the lowest t value is left out. This process
						it's called "backward elimination". <br> </p>


					<hr class="major" />



				</section>

			</div>
		</div>

		<!-- Sidebar -->
		<div id="sidebar">
			<div class="inner">

				<!-- Search -->
				<section id="search" class="alt">
					<form method="post" action="#">
						<input type="text" name="query" id="query" placeholder="Search" />
					</form>
				</section>
				<!-- Menu -->
				<nav id="menu">
					<header class="major">
						<h2>Menu</h2>
					</header>
					<ul>
						<li><a href="index.html">Home</a></li>
						<li><a href="introduccion.html">Introduction to Machine Learning </a></li>
						<li>
							<span class="opener" href="index.html">Theoretical framework</span>
							<ul>
								<li><a href="supervisados.html">Supervised learning</a></li>
								<ul>
									<li><a href="deRegresion.html">Regression</a></li>
									<li><a href="deClasificacion.html">Classification</a></li>
								</ul>
								<li><a href="noSupervisados.html">Unsupervised learning</a></li>
								<ul>
									<li><a href="deClustering.html">Clustering</a></li>
									<li><a href="deReduccionDim.html">Reduction of dimensions</a></li>

								</ul>
							</ul>
						</li>
						<li>
							<span class="opener" href="proyectos.html">Projects</span>
							<ul>
								<li><a href="proyectoUno.html">Home price prediction </a></li>
								<li><a href="proyectoDos.html">Cardiology prediction</a></li>

								<li><a href="proyectoCuatro.html"> Height prediction from skeletal remains</a></li>
								<li><a href="proyectoTres.html">Prediction of patients with / without liver</a></li>
								<li><a href="proyectoCinco.html">Prediction of autism in adults</a></li>



							</ul>
						</li>

					</ul>
				</nav>

			</div>
		</div>

	</div>

	<!-- Scripts -->
	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/browser.min.js"></script>
	<script src="assets/js/breakpoints.min.js"></script>
	<script src="assets/js/util.js"></script>
	<script src="assets/js/main.js"></script>

</body>

</html>