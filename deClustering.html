<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
	<title>Clustering </title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<link rel="stylesheet" href="assets/css/main.css" />
</head>

<body class="is-preload">

	<!-- Wrapper -->
	<div id="wrapper">

		<!-- Main -->
		<div id="main">
			<div class="inner">

				<!-- Header -->
				<header id="header">
					<a href="index.html" class="logo"></a>
					<ul class="icons">
						<li><a href="https://github.com/leticia8/portofliollado" class="icon brands fa-github"><span
									class="label">Git Hub</span></a></li>
						<li><a href="https://www.linkedin.com/in/leticia-lado-50b7a5106/"
								class="icon brands fa-linkedin"><span class="label">Linked In</span></a></li>
					</ul>
				</header>

				<!-- Content -->
				<section>
					<header class="main">
						<h1>Clustering algorithms</h1>
					</header>

					<span class="image main"><img src="images/unsupClus.png" style="width:80%;height:60%;"
							alt="" /></span>

					<p style="font-size:20px ;text-align:justify"> Clustering problems are those in which you want to
						discover the data groups that best apply to the problem
						that you want to solve, how to group customers by their buying behavior</p>

					<p style="font-size:20px ;text-align:justify"> Clustering techniques can be classified into:<br>

						<strong>According to ownership of the data: </strong> <br>
						Exclusive: each instance belongs to a single cluster. <br>
						Overlapping: they are not exclusive and a piece of information can belong to more than one
						cluster. <br>
						Hierarchical: each child can be joined to form a parent cluster. <br>
						Diffuse or probabilistic: each point belongs to all the clusters with degrees of membership
						ranging from 0 to 1. <br>

						<strong> According to the algorithm used to find the clusters: </strong> <br>
						Prototype-based: each cluster is represented by a central object (prototype). It is usually the
						center of the cluster - centroid. <br>
						Density cluster: A cluster can be defined as a dense region where objects are concentrated
						surrounded by an area of low density. Low-density areas can be ruled out as noise. <br>
						Hierarchical cluster: They are generated based on the distance between the points. The output is
						a dendrogram. There are two bottom-up approaches where each piece of data is considered a
						cluster and the clusters are joined and top-down.
						where the entire data set is a cluster and will be recursively divided into subclusters. It is
						useful when the size of the information is limited. <br>
						Model-based cluster: A cluster can be conceived as a group in which the points have the same
						probability of distribution. Each can be represented by a distribution model where the parameter
						distribution can be iteratively optimized. <br>



					<p style="font-size:24px ;color:Tomato;"> K-MEANS CLUSTERING</p>
					<span class="image main"><img src="images/k-means.png" style="width:50%;height:30%;"
							alt="" /></span>
					<p style="font-size:20px ;text-align:justify"> In this technique the user / researcher specifies the
						number of clusters k that should be grouped in the data set.
						The objective of k-means is to find a prototype for each cluster, all the points are then
						assigned to the closest prototype (centroid). <br>
						In k-means it is the mean of the points, but it could be the most representative information.
						The centroid does not have to be a real point in the data set, it can be imaginary. <br>
						Each partition of the data set is also called Voronoi partitions and each prototype is a seed in
						the partition, the points associated with the partition form a single partition. <br>
						K-Means clustering creates k partitions in n-dimensional space where n is the number of
						attributes in the data set. To partition the data set, a proximity measure must be used, the
						Euclidean distance is the most used. <br>
						You may have the problem of finding local optima rather than converging to the best global
						solution. <br> <br>
						<strong> ALGORITHM STEPS: </strong> <br>
						1- Start k centroids randomly. K must be specified by the user. <br>
						2- All points are assigned to the closest centroid to form a cluster. The most common distance
						is Euclidean but others such as Jaccard's coefficient or Manhattan distance can be used. <br>
						3- For each cluster, new centroids are calculated. The centroid is the most representative point
						of all the points in the cluster.
						This step can be interpreted as minimizing the sum of squared errors (SSE) of all points in the
						cluster to the centroid. <br>
						4 - Point assignment and centroid calculations are repeated until all points are reassigned to
						new centroids. <br>
						5-Terminates when no changes are identified to the assignment.
					</p>

					<p style="font-size:24px ;color:Tomato;">DBSCAN - Density Based Spatial Clustering of Applications
						with Noise</p>
					<span class="image main"><img src="images/dbscan.png" style="width:50%;height:30%;" alt="" /></span>
					<p style="font-size:20px ;text-align:justify"> A cluster can also be defined as an area of high data
						concentration (or density) surrounded by areas of low data concentration. <br>
						A density-based cluster identifies clusters in information based on the measure of density
						distribution in n-dimensional space. <br>
						You do not need to specify the number of cluster as a parameter.
						Density can be defined as the number of points in a unit of n-dimensional space. <br>
						Said n-dimensional space is determined by the number of attributes in the data set. At any point
						in the data set where there is a space of high density surrounded by another of relatively low
						density, a cluster is identified. <br>
						Attributes are preferable numeric because distance calculation is used. <br>
						The algorithm can be reduced to three steps: <br>
						1. Define a density threshold. <br>
						2. Classify the points. <br>
						3. Group them into clusters. <br>
						The algorithm starts with a denity calculation of all points in the data set given an Epsilon
						radius. A threshold is defined
						of points from which it is considered "high density". <br>
						The points are classified into three: <br>
						Core points: within the high-density region. <br>
						Edge points: located on the circumference or radius at an Epsilon distance. Soon considered
						within high-density space. <br>
						Noise points: It is not in either of the two previous classifications. <br>
						The groups of core points form different clusters. If two points considered core are at an
						Epsilon distance (or less) one of
						another belong to the same cluster. <br> </p>

					<p style="font-size:24px ;color:Tomato;">HIERARCHICAL CLUSTERING</p>
					<span class="image main"><img src="images/dendogramas.JPG" style="width:50%;height:30%;"
							alt="" /></span>
					<p style="font-size:20px ;text-align:justify"> It is an alternative to k-means clustering that does
						not require specifying a number of K. It additional benefit is that
						they result in a tree-shaped representation of the observations (dendogram) that makes them
						easily understandable. <br>
						There are bottom-up clustering that starts from the leaves and combines the clusters until
						reaching the trunk and top-down that performs the reverse process. <br>
						The earlier the mergers occur, the more the observation groups resemble each other. The height
						(considering the y-axis)
						in which two groups of observations come together indicates how different they are. We cannot
						make claims of how
						Similar are two observations based on their distance on the horizontal axis. <br>
						The hierarchical term refers to the fact that the clusters are obtained by "cutting" the
						dendrogram at any height. <br>
						The hierarchical clustering algorithm is very simple, it begins by defining a measure of
						dissimilarity between each pair of observations. <br>
						The Euclidean distance is the most used, then the algorithm proceeds iteratively. Starting with
						the leaves (in the bottom-up strategy), each observation
						is defined as a cluster. The two most similar clusters merge into n-1 clusters. The algorithm
						continues to proceed in the same way
						until there is a single cluster. <br>
						To apply the concept of dissimilarity to groups of observations, the concept of link is used.
						There are four types of link: <br>
						Complete: The maximum dissimilarity between the pairs of points in the clusters. <br>
						Simple: The minimum dissimilarity between the pairs of points in the clusters. <br>
						Average: the average of the differences between pairs of points of two clusters. <br>
						Centroid: dissimilarity between centroids of the clusters. <br>
						The most common are average, simple, and complete. Being average and complete preferred over
						simple since they result in
						more balanced dendograms. <br>
						<hr class="major" />

				</section>

			</div>
		</div>

		<!-- Sidebar -->
		<div id="sidebar">
			<div class="inner">

				<!-- Search -->
				<section id="search" class="alt">
					<form method="post" action="#">
						<input type="text" name="query" id="query" placeholder="Search" />
					</form>
				</section>
				<!-- Menu -->
				<nav id="menu">
					<header class="major">
						<h2>Menu</h2>
					</header>
					<ul>
						<li><a href="index.html">Home</a></li>
						<li><a href="introduccion.html">Introduction to Machine Learning </a></li>
						<li>
							<span class="opener" href="index.html">Theoretical framework</span>
							<ul>
								<li><a href="supervisados.html">Supervised learning</a></li>
								<ul>
									<li><a href="deRegresion.html">Regression</a></li>
									<li><a href="deClasificacion.html">Classification</a></li>
								</ul>
								<li><a href="noSupervisados.html">Unsupervised learning</a></li>
								<ul>
									<li><a href="deClustering.html">Clustering</a></li>
									<li><a href="deReduccionDim.html">Reduction of dimensions</a></li>

								</ul>
							</ul>
						</li>
						<li>
							<span class="opener" href="proyectos.html">Projects</span>
							<ul>
								<li><a href="proyectoUno.html">Home price prediction </a></li>
								<li><a href="proyectoDos.html">Cardiology prediction</a></li>

								<li><a href="proyectoCuatro.html"> Height prediction from skeletal remains</a></li>
								<li><a href="proyectoTres.html">Prediction of patients with / without liver</a></li>
								<li><a href="proyectoCinco.html">Prediction of autism in adults</a></li>



							</ul>
						</li>

					</ul>
				</nav>


			</div>
		</div>

	</div>

	</div>
	</div>

	</div>

	<!-- Scripts -->
	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/browser.min.js"></script>
	<script src="assets/js/breakpoints.min.js"></script>
	<script src="assets/js/util.js"></script>
	<script src="assets/js/main.js"></script>

</body>

</html>